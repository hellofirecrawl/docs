---
title: "Scrape"
description: "Turn any url into clean data"
og:title: "Scrape | Firecrawl"
og:description: "Turn any url into clean data"
---

import InstallationPython from "/snippets/v1/installation/python.mdx";
import InstallationNode from "/snippets/v1/installation/js.mdx";
import InstallationGo from "/snippets/v1/installation/go.mdx";
import InstallationRust from "/snippets/v1/installation/rust.mdx";
import ScrapePython from "/snippets/v1/scrape/base/python.mdx";
import ScrapeNode from "/snippets/v1/scrape/base/js.mdx";
import ScrapeGo from "/snippets/v1/scrape/base/go.mdx";
import ScrapeRust from "/snippets/v1/scrape/base/rust.mdx";
import ScrapeCURL from "/snippets/v1/scrape/base/curl.mdx";
import ScrapeResponse from "/snippets/v1/scrape/base/output.mdx";
import ExtractCURL from "/snippets/v1/llm-extract/base/curl.mdx";
import ExtractPython from "/snippets/v1/llm-extract/base/python.mdx";
import ExtractNode from "/snippets/v1/llm-extract/base/js.mdx";
import ExtractOutput from "/snippets/v1/llm-extract/base/output.mdx";
import ExtractNoSchemaCURL from "/snippets/v1/llm-extract/no-schema/curl.mdx";
import ExtractNoSchemaOutput from "/snippets/v1/llm-extract/no-schema/output.mdx";
import ScrapeActionsPython from "/snippets/v1/scrape/actions/python.mdx";
import ScrapeActionsNode from "/snippets/v1/scrape/actions/js.mdx";
import ScrapeActionsCURL from "/snippets/v1/scrape/actions/curl.mdx";
import ScrapeActionsOutput from "/snippets/v1/scrape/actions/output.mdx";
import BatchScrapePython from "/snippets/v1/batch-scrape/base/python.mdx";
import BatchScrapeNode from "/snippets/v1/batch-scrape/base/js.mdx";
import BatchScrapeCURL from "/snippets/v1/batch-scrape/base/curl.mdx";
import BatchScrapeOutput from "/snippets/v1/batch-scrape/base/output.mdx";
import BatchScrapeAsyncOutput from "/snippets/v1/batch-scrape/base/async-output.mdx";
import ScrapeLocationPython from "/snippets/v1/scrape/location/python.mdx";
import ScrapeLocationNode from "/snippets/v1/scrape/location/js.mdx";
import ScrapeLocationCURL from "/snippets/v1/scrape/location/curl.mdx";

## Getting Started with Scraping

<Info>
Firecrawl makes it easy to scrape web content by handling all the complex parts for you. Just provide a URL, and get back clean, usable data.
</Info>

### Quick Example

Here's a simple example to get started:

<CodeGroup>
```python Python
from firecrawl import FirecrawlApp

app = FirecrawlApp(api_key="YOUR_API_KEY")
result = app.scrape_url("https://example.com") 
print(result['markdown']) # Get the clean markdown content
```

```javascript JavaScript
import FirecrawlApp from '@mendable/firecrawl-js';

const app = new FirecrawlApp({apiKey: "YOUR_API_KEY"});
const result = await app.scrapeUrl('https://example.com');
console.log(result.markdown); // Get the clean markdown content
```

```bash cURL
curl -X POST https://api.firecrawl.dev/v1/scrape \
    -H 'Content-Type: application/json' \
    -H 'Authorization: Bearer YOUR_API_KEY' \
    -d '{"url": "https://example.com"}'
```
</CodeGroup>

## Installation

<CodeGroup>
<InstallationPython />
<InstallationNode />
<InstallationGo />
<InstallationRust />
</CodeGroup>

## Basic Scraping

The `/scrape` endpoint lets you extract content from any webpage in multiple formats. By default, it returns clean markdown perfect for LLM applications.

### Example Usage

<CodeGroup>
<ScrapePython />
<ScrapeNode />
<ScrapeGo />
<ScrapeRust />
<ScrapeCURL />
</CodeGroup>

### Response Format

<ScrapeResponse />

## Common Use Cases

### 1. Getting Structured Data (Extract)

Extract specific information from a webpage in a structured format:

<CodeGroup>
<ExtractPython />
<ExtractNode />
<ExtractCURL />
</CodeGroup>

Output:
<ExtractOutput />

<Tip>
Need a simpler way? You can extract data without defining a schema by just providing a prompt:
</Tip>

<CodeGroup>
<ExtractNoSchemaCURL />
</CodeGroup>

Output:
<ExtractNoSchemaOutput />

### 2. Batch Scraping Multiple URLs

Process multiple URLs at once:

<CodeGroup>
<BatchScrapePython />
<BatchScrapeNode />
<BatchScrapeCURL />
</CodeGroup>

<Accordion title="Response Examples">
Synchronous Response:
<BatchScrapeOutput />

Asynchronous Response:
<BatchScrapeAsyncOutput />
</Accordion>

## Advanced Features

<AccordionGroup>

<Accordion title="Interactive Actions">
Interact with web pages before scraping (e.g., clicking buttons, filling forms):

<CodeGroup>
<ScrapeActionsPython />
<ScrapeActionsNode />
<ScrapeActionsCURL />
</CodeGroup>

Example Output:
<ScrapeActionsOutput />
</Accordion>

<Accordion title="Location & Language Settings">
Customize the scraping location and language:

<CodeGroup>
<ScrapeLocationPython />
<ScrapeLocationNode />
<ScrapeLocationCURL />
</CodeGroup>
</Accordion>

</AccordionGroup>

## Additional Resources

<CardGroup cols={2}>
<Card title="API Reference" icon="book" href="/api-reference/endpoint/scrape">
Detailed API documentation with all available parameters
</Card>
<Card title="Advanced Guide" icon="graduation-cap" href="/advanced-scraping-guide">
Learn advanced scraping techniques and options
</Card>
</CardGroup>