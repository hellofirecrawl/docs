---
title: 'Crawl'
description: 'Automatically crawl and extract content from entire websites'
og:title: "Crawl | Firecrawl"
icon: "spider"
og:description: "Learn how to crawl websites and extract data automatically with Firecrawl"
---

import InstallationPython from "/snippets/v1/installation/python.mdx";
import InstallationNode from "/snippets/v1/installation/js.mdx";
import InstallationGo from "/snippets/v1/installation/go.mdx";
import InstallationRust from "/snippets/v1/installation/rust.mdx";
import CrawlPython from "/snippets/v1/crawl/base/python.mdx";
import CrawlNode from "/snippets/v1/crawl/base/js.mdx";
import CrawlGo from "/snippets/v1/crawl/base/go.mdx";
import CrawlRust from "/snippets/v1/crawl/base/rust.mdx";
import CrawlCURL from "/snippets/v1/crawl/base/curl.mdx";
import AsyncCrawlOutput from "/snippets/v1/crawl-async/base/output.mdx";
import CheckCrawlJobPython from "/snippets/v1/crawl-status/short/python.mdx";
import CheckCrawlJobNode from "/snippets/v1/crawl-status/short/js.mdx";
import CheckCrawlJobGo from "/snippets/v1/crawl-status/short/go.mdx";
import CheckCrawlJobRust from "/snippets/v1/crawl-status/short/rust.mdx";
import CheckCrawlJobCURL from "/snippets/v1/crawl-status/short/curl.mdx";
import CheckCrawlJobOutputScraping from "/snippets/v1/crawl-status/base/output-scraping.mdx";
import CheckCrawlJobOutputCompleted from "/snippets/v1/crawl-status/base/output-completed.mdx";
import CrawlWebSocketPython from "/snippets/v1/crawl-websocket/base/python.mdx";
import CrawlWebSocketNode from "/snippets/v1/crawl-websocket/base/js.mdx";
import CrawlWebhookCURL from "/snippets/v1/crawl-webhook/base/curl.mdx";

## What is Website Crawling?

Website crawling is the automated process of systematically browsing through a website's pages to collect data. With Firecrawl, you can:

1. Start with any URL on a website
2. Automatically discover and visit all linked pages
3. Extract clean, structured content from each page
4. Get the data in formats ready for your applications

## Getting Started

### Installation 

First, install Firecrawl in your preferred language:

<CodeGroup>
<InstallationPython />
<InstallationNode />
<InstallationGo />
<InstallationRust />
</CodeGroup>

### Basic Crawling

Here's a simple example to crawl a website and get its content:

<CodeGroup>
<CrawlPython />
<CrawlNode />
<CrawlGo />
<CrawlRust />
<CrawlCURL />
</CodeGroup>

<Note>
By default, Firecrawl only crawls pages within the same directory path as your starting URL. For example, if you start at `website.com/blog/`, it won't crawl `website.com/products/`. Use `allowBackwardLinks: true` to crawl the entire site.
</Note>

### Checking Crawl Progress

When you start a crawl, you'll get a crawl ID. Use this to check the status of your crawl:

<CodeGroup>
<CheckCrawlJobPython />
<CheckCrawlJobNode />
<CheckCrawlJobGo />
<CheckCrawlJobRust />
<CheckCrawlJobCURL />
</CodeGroup>

The response will show you:
- How many pages have been crawled
- The content from each page
- Whether the crawl is still in progress

## Advanced Features 

### Real-time Updates with WebSocket

Get instant updates as pages are crawled using WebSockets:

<CodeGroup>
<CrawlWebSocketPython />
<CrawlWebSocketNode />
</CodeGroup>

### Webhook Integration

Receive notifications about crawl progress to your server:

<CrawlWebhookCURL />

Webhook events include:
- `crawl.started`: When crawling begins
- `crawl.page`: Each time a page is crawled
- `crawl.completed`: When crawling finishes (Beta)
- `crawl.failed`: If an error occurs

<Note>
In rare cases, you might receive the `crawl.completed` event while some final `crawl.page` events are still processing.
</Note>

## Response Handling

### For Ongoing Crawls
<CheckCrawlJobOutputScraping />

### For Completed Crawls
<CheckCrawlJobOutputCompleted />

<Info>
When using our SDKs, pagination with `next` and `skip` parameters is handled automatically. You'll just get all results at once.
</Info>

## Common Use Cases

1. **Documentation Sites**
   ```js
   // Crawl all documentation pages
   await app.crawlUrl('docs.example.com', {
     includePaths: ['/docs/*'],
     excludePaths: ['/docs/archived/*']
   });
   ```

2. **Blog Archives**
   ```js
   // Crawl blog posts from specific years
   await app.crawlUrl('blog.example.com', {
     includePaths: ['/2023/*', '/2024/*']
   });
   ```

3. **Product Catalogs**
   ```js
   // Crawl product pages with screenshots
   await app.crawlUrl('store.example.com', {
     includePaths: ['/products/*'],
     scrapeOptions: {
       formats: ['markdown', 'screenshot']
     }
   });
   ```